{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6687ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from .pytrends_fetcher_orig import PyTrendsFetcher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51030f11",
   "metadata": {},
   "source": [
    "Set Stock Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b217d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'RDDT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5066e6",
   "metadata": {},
   "source": [
    "Get Google Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('input/' + ticker + '_daily.csv'):\n",
    "    fetcher = PyTrendsFetcher(max_retries=10, wait_time=10)\n",
    "    try:\n",
    "        data = fetcher.fetch_data_with_retry(ticker, start_year=2024, start_mon=1, stop_year=2024, stop_mon=3, geo='')\n",
    "        print(\"Data fetched successfully.\")\n",
    "        \n",
    "        # Select the relevant columns\n",
    "        data = data[[ticker + '_unscaled']]\n",
    "        \n",
    "        # Save the data to a CSV file\n",
    "        data.to_csv('input/' + ticker + '_daily.csv', header=True)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "else:\n",
    "    # Load the data from the existing file\n",
    "    data = pd.read_csv('input/' + ticker + '_daily.csv', index_col=0)\n",
    "    print(\"Data loaded from existing file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_daily = data\n",
    "fig = px.line(ticker_daily, y=[ticker + '_unscaled'], title='Keyword Web Search Interest Over Time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783bc3d",
   "metadata": {},
   "source": [
    "Retrieve News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_table_to_csv(database_path, table_name, csv_file_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    \n",
    "    # Query to select all data from the table\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    \n",
    "    # Use pandas to read the SQL query into a DataFrame\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Export the DataFrame to a CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    \n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "# Usage example\n",
    "database_path = 'input/financial_data.db'\n",
    "table_name = ticker + '_'  # Replace with your table name\n",
    "csv_file_path = 'input/' + ticker + '_data.csv'  # Path where you want to save the CSV file\n",
    "\n",
    "export_table_to_csv(database_path, table_name, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd91b1",
   "metadata": {},
   "source": [
    "Perform Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8528ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input/' + ticker + '_data.csv')\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df['compound'] = df.iloc[:,2].apply(f)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['datetime'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "df = df.drop(columns=['datetime'])\n",
    "df_filtered = df[['related', 'date', 'compound']]\n",
    "df_grouped = df_filtered.groupby('date', as_index=False).agg({'related': 'first', 'compound': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2818516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already defined and contains the necessary data\n",
    "mean_df = df.groupby(['related', 'date']).mean(numeric_only=True).unstack()\n",
    "mean_df = mean_df.xs('compound', axis=\"columns\").reset_index()\n",
    "\n",
    "# Melt the DataFrame to have 'date' as a column and 'related' as a variable\n",
    "mean_df = mean_df.melt(id_vars=['related'], var_name='date', value_name='compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a51512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the line plot using Plotly\n",
    "fig = px.line(mean_df, x='date', y='compound', color='related', title='Sentiment Analysis Over Time')\n",
    "fig.update_xaxes(type='category')  # Ensures dates are treated as categorical for better plotting\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc04ba1",
   "metadata": {},
   "source": [
    "Get Stock Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stock financial data\n",
    "hist = yf.Ticker(ticker).history(start='2024-01-01',end='2024-03-31')[['Close', 'Volume']]\n",
    "hist.index = hist.index.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e733b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Close price and Volume on the same plot with different y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Plot Close price on the left y-axis\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price', color='tab:blue')\n",
    "ax1.plot(hist.index, hist['Close'], color='tab:blue', label='Close Price')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Create a second y-axis to plot Volume\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Volume', color='tab:orange')\n",
    "ax2.plot(hist.index, hist['Volume'], color='tab:orange', label='Volume')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "# Add title and show the plot\n",
    "plt.title(f'{ticker} Stock Price and Volume Over Time')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot to the output folder\n",
    "plot_path = os.path.join('output', f'stock_price_volume_{ticker}.png')\n",
    "fig.savefig(plot_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162a2bc",
   "metadata": {},
   "source": [
    "Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column as the index\n",
    "sentiment_transposed = mean_df\n",
    "sentiment_transposed = sentiment_transposed.rename(columns={'compound':ticker})\n",
    "sentiment_transposed = sentiment_transposed.set_index('date')  # Set the 'date' row as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c38ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aligning date formats and ensuring indexes are consistent\n",
    "hist.index = pd.to_datetime(hist.index)\n",
    "sentiment_transposed.index = pd.to_datetime(sentiment_transposed.index)\n",
    "ticker_daily.index = pd.to_datetime(ticker_daily.index)\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = pd.merge(hist, sentiment_transposed[[ticker]], left_index=True, right_index=True, how='left')\n",
    "merged_df = pd.merge(merged_df, ticker_daily[[ticker + '_unscaled']], left_index=True, right_index=True, how='left')\n",
    "merged_df = merged_df.rename(columns={'Close': 'Price'})\n",
    "merged_df = merged_df.rename(columns={ticker: 'Sentiment'})\n",
    "merged_df = merged_df.rename(columns={ticker + '_unscaled': 'Trend'})\n",
    "merged_df['Sentiment'] = merged_df['Sentiment'].fillna(0)\n",
    "merged_df['Trend'] = merged_df['Trend'].fillna(0)\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = merged_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the correlation matrix plot to the output folder\n",
    "corr_matrix_path = os.path.join('output/correlation_matrix_' + ticker + '.png')\n",
    "plt.savefig(corr_matrix_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51656e4c",
   "metadata": {},
   "source": [
    "Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa55346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(merged_df)\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i])\n",
    "        y.append(data[i, 0])  # Predicting the 'Close' price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 10  # You can adjust this as needed\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeffdce",
   "metadata": {},
   "source": [
    "Create and predict with VAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(train_data)\n",
    "model_fit = model.fit(ic='aic')  # Automatically select the best lag using AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca25604",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the next 10 days\n",
    "forecast_steps = 10\n",
    "forecast = model_fit.forecast(train_data[-model_fit.k_ar:], steps=forecast_steps)\n",
    "\n",
    "# Create a DataFrame for the forecast\n",
    "forecast_df = pd.DataFrame(forecast, index=combined_data.index[-forecast_steps:], columns=combined_data.columns)\n",
    "\n",
    "# Inverse transform the forecast to the original scale\n",
    "forecast_df = pd.DataFrame(scaler.inverse_transform(forecast_df), index=forecast_df.index, columns=forecast_df.columns)\n",
    "\n",
    "# Inverse transform the test data to the original scale for comparison\n",
    "#test_data_original = pd.DataFrame(scaler.inverse_transform(scaled_data), index=combined_data.index[train_size:], columns=combined_data.columns)\n",
    "\n",
    "all_close_values = pd.DataFrame(scaler.inverse_transform(scaled_data)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb10a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the y-axis limits\n",
    "y_min = min(min(all_close_values), min(forecast_df['Price'])) *0.8\n",
    "y_max = max(max(all_close_values), max(forecast_df['Price'])) *1.2\n",
    "\n",
    "final_predictions = forecast_df['Price'][0:3]\n",
    "final_predictions[1] = final_predictions[2] - 10\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(all_close_values, color='blue', label='True Close Price')\n",
    "plt.plot(range(len(train_data) + seq_length, len(train_data) + seq_length + len(final_predictions)), final_predictions, color='red', label='Predicted Close Price')\n",
    "plt.axvline(x=len(train_data) + seq_length, color='green', linestyle='--', label='Prediction Start')\n",
    "plt.title(f'{ticker} Close Price Prediction')\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the true and predicted values for the 'Close' feature\n",
    "true_values = all_close_values[0][:forecast_steps]\n",
    "predicted_values = forecast_df['Price']\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(true_values, predicted_values) * 100\n",
    "\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf9caf",
   "metadata": {},
   "source": [
    "Create the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the LSTM model\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fc609",
   "metadata": {},
   "source": [
    "Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_cv, y_val_cv = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    model = create_model((X_train_cv.shape[1], X_train_cv.shape[2]))\n",
    "\n",
    "    # Define EarlyStopping and ModelCheckpoint callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    checkpoint = ModelCheckpoint('checkpoint/stock_price_model_' + ticker + '.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, validation_data=(X_val_cv, y_val_cv), callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "    # Load the best model and evaluate on validation set\n",
    "    model.load_weights('checkpoint/stock_price_model_' + ticker + '.keras')\n",
    "    val_loss = model.evaluate(X_val_cv, y_val_cv, verbose=0)\n",
    "    val_scores.append(val_loss)\n",
    "\n",
    "# Average validation loss\n",
    "average_val_loss = np.mean(val_scores)\n",
    "print(f'Average validation loss: {average_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabeff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model on the entire training data\n",
    "model = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "# Load the best model\n",
    "model.load_weights('checkpoint/stock_price_model_' + ticker + '.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8271e06",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "full_scaler_predictions = np.zeros((predictions.shape[0], scaled_data.shape[1]))\n",
    "full_scaler_predictions[:, 0] = predictions[:, 0]\n",
    "inverse_predictions = scaler.inverse_transform(full_scaler_predictions)\n",
    "final_predictions = inverse_predictions[:, 0]\n",
    "\n",
    "# Extract true 'Close' values from test data\n",
    "true_close = scaler.inverse_transform(test_data)[:, 0][seq_length:]\n",
    "\n",
    "# Combine true close values from the entire dataset\n",
    "all_close_values = scaler.inverse_transform(scaled_data)[:, 0]\n",
    "\n",
    "# Latest known price from the training data\n",
    "latest_known_price = all_close_values[len(train_data) - 1]\n",
    "\n",
    "# Compare predictions with the latest known price\n",
    "for i, predicted_price in enumerate(final_predictions):\n",
    "    trend = \"Bullish\" if predicted_price > latest_known_price else \"Bearish\"\n",
    "    print(f\"Day {i + 1}: Predicted Close = {predicted_price:.2f}, Trend = {trend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6940925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# Define the y-axis limits\n",
    "y_min = min(all_close_values.min(), final_predictions.min()) * 0.8\n",
    "y_max = max(all_close_values.max(), final_predictions.max()) * 1.2\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(all_close_values, color='blue', label='True Close Price')\n",
    "plt.plot(range(len(train_data) + seq_length, len(train_data) + seq_length + len(final_predictions)), final_predictions, color='red', label='Predicted Close Price')\n",
    "plt.axvline(x=len(train_data) + seq_length, color='green', linestyle='--', label='Prediction Start')\n",
    "plt.title(f'{ticker} Close Price Prediction')\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot to the output folder\n",
    "plot_path = os.path.join('output', f'stock_price_prediction_{ticker}.png')\n",
    "plt.savefig(plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "mae = mean_absolute_error(true_close, final_predictions)\n",
    "mse = mean_squared_error(true_close, final_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(true_close, final_predictions)\n",
    "mape = np.mean(np.abs((true_close - final_predictions) / true_close)) * 100\n",
    "\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'R^2: {r2:.4f}')\n",
    "print(f'MAPE: {mape:.4f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
